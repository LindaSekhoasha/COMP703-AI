{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D__YNfprA2Ir"
   },
   "source": [
    "# Machine Translation (Seq2Seq) - RNN and Attention\n",
    "Members\n",
    "* Linda Sekhoasha - 222004139\n",
    "* Wandile Ngobese - 222056013\n",
    "* Khonzinkosi Mkhize - 219005273\n",
    "* Samukelo Mkhize - 220009930"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QxQ-Ht9vJCng"
   },
   "source": [
    "## Pip Commands & Imports (run once per runtime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core utilities\n",
    "!pip install pandas\n",
    "\n",
    "# Matching NLP/ML stack\n",
    "!pip install evaluate datasets==2.18.0 transformers==4.39.3 sentencepiece==0.1.99\n",
    "\n",
    "# English tokenizer for spaCy\n",
    "!python -m spacy download en_core_web_sm\n",
    "\n",
    "# Downgrade NumPy for compatibility with evaluate/datasets\n",
    "!pip uninstall -y numpy\n",
    "!pip install numpy==2.3.0\n",
    "\n",
    "# Clean install of core torch packages\n",
    "!pip uninstall -y torch torchtext torchaudio torchvision\n",
    "!pip install torch==2.2.0 torchaudio==2.2.0 torchvision==0.17.0 torchtext==0.17.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(\"Torch version:\", torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "print(\"TorchText version:\", torchtext.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "print(\"TorchText version:\", torchvision.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "print(\"TorchText version:\", torchaudio.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "print(np.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library\n",
    "import string\n",
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "# Scientific computing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchtext\n",
    "from torchtext.vocab import Vocab, build_vocab_from_iterator\n",
    "\n",
    "# Hugging Face datasets, evaluation & tranfsormer model\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer\n",
    "\n",
    "# NLP\n",
    "import spacy\n",
    "\n",
    "# Progress bar\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load English tokenizer (used for both English and Zulu for now)\n",
    "en_nlp = spacy.load(\"en_core_web_sm\")\n",
    "zu_nlp = en_nlp  # TODO: Replace with actual Zulu tokenizer if available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "10f755cf-8361-4f02-a559-3890565dd43a"
   },
   "source": [
    "# Data Preparation\n",
    "Used Xhosa Corpus [XhosaNavy](https://opus.nlpl.eu/XhosaNavy/en&xh/v1/XhosaNavy). Xhosa is similar to Zulu, so the semantics are the same. The dataset was preferred as it is cleaner than any [Zulu](https://huggingface.co/datasets/LindaSekhoasha/zu-en_parallel-corpus_xsm) datasets we could find in the time constraint we had."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load parallel corpus (Xhosa ↔ English) from Hugging Face\n",
    "dataset = datasets.load_dataset(\"LindaSekhoasha/xh-en_parallel_corpus\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DjoImsVbMRzC"
   },
   "source": [
    "## Dataset Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split 'train' into 80% training and 20% temporary test/validation\n",
    "split_dataset = dataset[\"train\"].train_test_split(test_size=0.2, seed=42)\n",
    "\n",
    "# Split the 20% into 50% validation and 50% test (i.e., 10% each of total)\n",
    "test_valid = split_dataset[\"test\"].train_test_split(test_size=0.5, seed=42)\n",
    "\n",
    "# Final dataset assignments\n",
    "train_data = split_dataset[\"train\"]\n",
    "valid_data = test_valid[\"train\"]\n",
    "test_data = test_valid[\"test\"]\n",
    "\n",
    "# Confirm dataset structure\n",
    "print(train_data.features)\n",
    "print(valid_data.features)\n",
    "print(test_data.features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HFeqdjW1OpC_"
   },
   "source": [
    "## Tokenize Function\n",
    "\n",
    "The `tokenize_example` function processes each parallel example from the dataset by:\n",
    "\n",
    "- Tokenizing the English (`\"en\"`) and Xhosa/Zulu (`\"xh\"`) text using SpaCy.\n",
    "- Truncating each token list to a maximum length.\n",
    "- Optionally converting tokens to lowercase.\n",
    "- Adding special `<sos>` (start-of-sentence) and `<eos>` (end-of-sentence) tokens.\n",
    "- Returning a dictionary containing the tokenized output for both languages: `{\"en_tokens\": ..., \"zu_tokens\": ...}`.\n",
    "\n",
    "This prepares the text for vocabulary building and numerical encoding in a sequence-to-sequence model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_example(example, en_nlp, zu_nlp, max_length, lower, sos_token, eos_token):\n",
    "    en_tokens = [token.text for token in en_nlp.tokenizer(example[\"en\"])][:max_length]\n",
    "    zu_tokens = [token.text for token in zu_nlp.tokenizer(example[\"xh\"])][:max_length]\n",
    "    if lower:\n",
    "        en_tokens = [token.lower() for token in en_tokens]\n",
    "        zu_tokens = [token.lower() for token in zu_tokens]\n",
    "    en_tokens = [sos_token] + en_tokens + [eos_token]\n",
    "    zu_tokens = [sos_token] + zu_tokens + [eos_token]\n",
    "    return {\"en_tokens\": en_tokens, \"zu_tokens\": zu_tokens}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 1_000\n",
    "lower = True\n",
    "sos_token = \"<sos>\"\n",
    "eos_token = \"<eos>\"\n",
    "\n",
    "fn_kwargs = {\n",
    "    \"en_nlp\": en_nlp,\n",
    "    \"zu_nlp\": zu_nlp,\n",
    "    \"max_length\": max_length,\n",
    "    \"lower\": lower,\n",
    "    \"sos_token\": sos_token,\n",
    "    \"eos_token\": eos_token,\n",
    "}\n",
    "\n",
    "# Apply the tokenization\n",
    "train_data = train_data.map(tokenize_example, fn_kwargs=fn_kwargs)\n",
    "valid_data = valid_data.map(tokenize_example, fn_kwargs=fn_kwargs)\n",
    "test_data = test_data.map(tokenize_example, fn_kwargs=fn_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show first example\n",
    "train_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HMqM3tGoREvW"
   },
   "source": [
    "## Vocabulary Construction and Special Token Handling\n",
    "\n",
    "This block builds separate vocabularies for English and Xhosa/Zulu token sequences using the training data.\n",
    "- Only tokens appearing at least twice (`min_freq = 2`) are included.\n",
    "- Special tokens `<unk>` (unknown) and `<pad>` (padding) are added to both vocabularies.\n",
    "- It asserts that both vocabularies assign the same indices to these special tokens — this ensures consistency during padding, masking, and embedding lookup.\n",
    "- Finally, it stores the indices of `<unk>` and `<pad>` for later use in model input preparation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocab Parameters\n",
    "min_freq = 2\n",
    "unk_token = \"<unk>\"\n",
    "pad_token = \"<pad>\"\n",
    "\n",
    "special_tokens = [unk_token, pad_token]\n",
    "\n",
    "# Vocab Construction\n",
    "en_vocab = torchtext.vocab.build_vocab_from_iterator(\n",
    "    train_data[\"en_tokens\"],\n",
    "    min_freq=min_freq,\n",
    "    specials=special_tokens,\n",
    ")\n",
    "\n",
    "zu_vocab = torchtext.vocab.build_vocab_from_iterator(\n",
    "    train_data[\"zu_tokens\"],\n",
    "    min_freq=min_freq,\n",
    "    specials=special_tokens,\n",
    ")\n",
    "\n",
    "# Ensure Special Token Index Consistency\n",
    "assert en_vocab[unk_token] == zu_vocab[unk_token], \"Mismatch in <unk> token index\"\n",
    "assert en_vocab[pad_token] == zu_vocab[pad_token], \"Mismatch in <pad> token index\"\n",
    "\n",
    "# Store Special Token Indices\n",
    "unk_index = en_vocab[unk_token]\n",
    "pad_index = en_vocab[pad_token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the default index for unknown tokens (important!)\n",
    "'''\n",
    "This step ensures that any out-of-vocabulary (OOV) tokens encountered\n",
    "during token-to-index conversion are automatically mapped to the '<unk>' index.\n",
    "'''\n",
    "en_vocab.set_default_index(unk_index)\n",
    "zu_vocab.set_default_index(unk_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GPWL4TDISxSd"
   },
   "source": [
    "## Token-to-Index Conversion\n",
    "\n",
    "The `numericalize_example` function maps tokenized words (`en_tokens`, `zu_tokens`) to their corresponding integer indices using the vocabularies.\n",
    "\n",
    "This prepares the sequences for conversion to tensors and eventual input to the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to convert tokens in vocab to indices\n",
    "def numericalize_example(example, en_vocab, zu_vocab):\n",
    "    en_ids = en_vocab.lookup_indices(example[\"en_tokens\"])\n",
    "    zu_ids = zu_vocab.lookup_indices(example[\"zu_tokens\"])\n",
    "    return {\"en_ids\": en_ids, \"zu_ids\": zu_ids}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LT_FghIzR736"
   },
   "source": [
    "## Numericalization of Tokens\n",
    "\n",
    "This step converts tokenized words into integer indices using the previously constructed vocabularies.\n",
    "\n",
    "- Each token list (`en_tokens`, `zu_tokens`) is mapped to its corresponding list of indices.\n",
    "- This is necessary to feed sequences into the neural network, which operates on numerical inputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_kwargs = {\"en_vocab\": en_vocab, \"zu_vocab\": zu_vocab}\n",
    "\n",
    "train_data = train_data.map(numericalize_example, fn_kwargs=fn_kwargs)\n",
    "valid_data = valid_data.map(numericalize_example, fn_kwargs=fn_kwargs)\n",
    "test_data = test_data.map(numericalize_example, fn_kwargs=fn_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the new features in an example (zu_ids and en_ids)\n",
    "train_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JMuWWBrUSe7t"
   },
   "source": [
    "## Dataset Formatting for PyTorch\n",
    "\n",
    "This step converts the numericalized token sequences (`en_ids`, `zu_ids`) into PyTorch tensors.\n",
    "\n",
    "- The `.with_format()` method enables seamless use with PyTorch-based DataLoaders and models.\n",
    "- Setting `output_all_columns=True` keeps access to original fields like raw text and tokens.\n",
    "- We verify that the transformation worked by checking the type of a sample sequence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract raw data (avoids NumPy formatting issues)\n",
    "train_raw = train_data.with_format(\"python\")\n",
    "valid_raw = valid_data.with_format(\"python\")\n",
    "test_raw = test_data.with_format(\"python\")\n",
    "\n",
    "# Define tensor conversion function (only updates relevant fields)\n",
    "def to_tensor(original, converted):\n",
    "    original[\"en_ids\"] = converted[\"en_ids\"]\n",
    "    original[\"zu_ids\"] = converted[\"zu_ids\"]\n",
    "    original[\"attention_mask\"] = converted[\"attention_mask\"]\n",
    "    return original\n",
    "\n",
    "# Create tensor versions of en_ids, zu_ids, attention_mask\n",
    "def convert(example):\n",
    "    return {\n",
    "        \"en_ids\": torch.tensor(example[\"en_ids\"], dtype=torch.long),\n",
    "        \"zu_ids\": torch.tensor(example[\"zu_ids\"], dtype=torch.long),\n",
    "        \"attention_mask\": torch.tensor(example[\"attention_mask\"], dtype=torch.long)\n",
    "        if \"attention_mask\" in example else None,\n",
    "    }\n",
    "\n",
    "train_tensor_fields = [convert(example) for example in train_raw]\n",
    "valid_tensor_fields = [convert(example) for example in valid_raw]\n",
    "test_tensor_fields = [convert(example) for example in test_raw]\n",
    "\n",
    "# Merge tensors into full original examples\n",
    "train_data = [to_tensor(orig, tens) for orig, tens in zip(train_raw, train_tensor_fields)]\n",
    "valid_data = [to_tensor(orig, tens) for orig, tens in zip(valid_raw, valid_tensor_fields)]\n",
    "test_data  = [to_tensor(orig, tens) for orig, tens in zip(test_raw,  test_tensor_fields)]\n",
    "\n",
    "# Final check\n",
    "print(type(train_data[0][\"en_ids\"]))  # <class 'torch.Tensor'>\n",
    "print(train_data[0].keys())           # should include en_ids, zu_ids, attention_mask + all original fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PqMircnOTWhM"
   },
   "source": [
    "## Custom Collate Function for Batching\n",
    "\n",
    "This function returns a `collate_fn` used by PyTorch DataLoaders to:\n",
    "\n",
    "- Collect a batch of individual examples\n",
    "- Pad each sequence (`en_ids`, `zu_ids`) to the same length in the batch\n",
    "- Return a dictionary of padded sequences ready for model input\n",
    "\n",
    "Padding ensures that all sequences in a batch are of equal length, which is required for efficient GPU processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create a custom collate_fn for padding sequences in a batch\n",
    "def get_collate_fn(pad_index):\n",
    "    def collate_fn(batch):\n",
    "        # Extract sequences\n",
    "        batch_en_ids = [example[\"en_ids\"] for example in batch]\n",
    "        batch_zu_ids = [example[\"zu_ids\"] for example in batch]\n",
    "\n",
    "        # Pad sequences\n",
    "        batch_en_ids = nn.utils.rnn.pad_sequence(batch_en_ids, padding_value=pad_index)\n",
    "        batch_zu_ids = nn.utils.rnn.pad_sequence(batch_zu_ids, padding_value=pad_index)\n",
    "\n",
    "        return {\n",
    "            \"en_ids\": batch_en_ids,\n",
    "            \"zu_ids\": batch_zu_ids,\n",
    "        }\n",
    "\n",
    "    return collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a PyTorch DataLoader with custom padding-based collate function\n",
    "def get_data_loader(dataset, batch_size, pad_index, shuffle=False):\n",
    "    collate_fn = get_collate_fn(pad_index)\n",
    "    data_loader = torch.utils.data.DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=batch_size,\n",
    "        collate_fn=collate_fn,\n",
    "        shuffle=shuffle,\n",
    "    )\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders\n",
    "# Note: larger batch size needs more GPU power but trains faster\n",
    "batch_size = 128\n",
    "\n",
    "train_data_loader = get_data_loader(train_data, batch_size, pad_index, shuffle=True)\n",
    "valid_data_loader = get_data_loader(valid_data, batch_size, pad_index)\n",
    "test_data_loader = get_data_loader(test_data, batch_size, pad_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4OXTZRCBAiq9"
   },
   "source": [
    "# RNN Model (LSTM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y5YyhVDnZxFz"
   },
   "source": [
    "## Encoder Architecture\n",
    "\n",
    "The encoder module of our sequence-to-sequence architecture is implemented using a multi-layer Long Short-Term Memory (LSTM) network. Its role is to process the source language sequence (Xhosa or Zulu) and compress it into a fixed-dimensional hidden state that captures the sequence’s semantic and syntactic properties.\n",
    "\n",
    "Each token is first mapped to a dense vector via an embedding layer. A dropout layer is applied for regularization, reducing the risk of overfitting during training. The embedded token sequence is then passed through a stack of LSTM layers, which update their hidden and cell states at each time step based on the current input and the past state. Only the final hidden and cell states from the topmost LSTM layer are retained — these summarize the entire input sequence and are used to initialize the decoder.\n",
    "\n",
    "We implement this logic in code by defining an `Encoder` class that inherits from `torch.nn.Module`, a base class for all neural network components in PyTorch. Within the `__init__()` method, we initialize all necessary layers and configurations. This includes an embedding layer, a dropout module, and a multi-layer LSTM. These components are constructed using `nn.Embedding`, `nn.Dropout`, and `nn.LSTM`, respectively. The `super().__init__()` call ensures that the module properly inherits PyTorch functionality.\n",
    "\n",
    "The encoder accepts the following hyperparameters:\n",
    "\n",
    "- `input_dim`: the size of the input vocabulary. This also corresponds to the dimensionality of one-hot vectors used to index into the embedding table.\n",
    "- `embedding_dim`: the size of the dense vectors that replace one-hot encodings. These embeddings capture semantic similarity between words in a lower-dimensional space.\n",
    "- `hidden_dim`: the dimensionality of the hidden and cell states used by the LSTM. This determines the model’s capacity to capture temporal dependencies.\n",
    "- `n_layers`: the number of LSTM layers in the encoder stack. More layers typically capture deeper contextual information.\n",
    "- `dropout`: a regularization parameter controlling the dropout probability, applied to the embedding vectors before they enter the LSTM.\n",
    "\n",
    "Although we do not go into detail about word embeddings in this section, it is worth noting that embeddings are a fundamental mechanism for representing words in continuous vector space. These vectors are typically learned during training and provide more meaningful representations than sparse one-hot vectors. For further reading on word embeddings, see Jurafsky and Martin (2021), or explore popular tutorials on Word2Vec, GloVe, and contextual embeddings.\n",
    "\n",
    "This architecture follows the standard sequence-to-sequence design pattern described in Jurafsky and Martin (2021), where recurrent neural networks encode the temporal structure of a sentence into a context vector. The use of multi-layer LSTMs with dropout reflects best practices in deep learning for NLP, as advocated in foundational teachings by Andrew Ng and others.\n",
    "\n",
    "**Reference:**  \n",
    "Jurafsky, D., & Martin, J. H. (2021). *Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics and Speech Recognition* (3rd ed.). Prentice Hall.  \n",
    "Andrew Ng, *Deep Learning Specialization*, Stanford University / DeepLearning.AI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
    "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src):\n",
    "        # src = [src length, batch size]\n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        # embedded = [src length, batch size, embedding dim]\n",
    "        outputs, (hidden, cell) = self.rnn(embedded)\n",
    "        # outputs = [src length, batch size, hidden dim * n directions]\n",
    "        # hidden = [n layers * n directions, batch size, hidden dim]\n",
    "        # cell = [n layers * n directions, batch size, hidden dim]\n",
    "        # outputs are always from the top hidden layer\n",
    "        return hidden, cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a5891167-c349-45bd-ba70-d276a38771b5"
   },
   "source": [
    "## Decoder Architecture\n",
    "\n",
    "The decoder is the second half of the sequence-to-sequence architecture, responsible for generating the target language sentence (in this case, English) one token at a time. Like the encoder, it is implemented as a multi-layer Long Short-Term Memory (LSTM) network, but with key differences that enable autoregressive generation.\n",
    "\n",
    "The decoder receives as input a single token at each time step — either the `<sos>` token at the start of decoding or the previous token generated during training or inference. The decoder maintains an internal hidden state and cell state, both of which are initialized using the final hidden and cell states from the encoder. This allows the decoder to condition its outputs on the full encoded representation of the source sequence.\n",
    "\n",
    "Each input token is mapped to a dense vector via an embedding layer. A dropout layer is applied to the embeddings for regularization. These vectors are then passed through the LSTM network, which updates the hidden and cell states. The output from the LSTM is passed through a fully connected linear layer to project the hidden state to the size of the target vocabulary. The result is a vector of logits from which the most probable next token can be selected.\n",
    "\n",
    "This process is repeated sequentially for each token in the output sequence. In contrast to the encoder, the decoder operates in a step-wise manner and returns both the prediction (logits over the target vocabulary) and the updated hidden and cell states at every time step. This recurrent setup is crucial for autoregressive decoding, allowing the model to build up a coherent output sequence based on past outputs and the source context.\n",
    "\n",
    "In code, the decoder is implemented as a subclass of `torch.nn.Module`. It takes the following hyperparameters:\n",
    "\n",
    "- `output_dim`: the size of the target vocabulary.\n",
    "- `embedding_dim`: the dimensionality of the learned word embeddings for the target language.\n",
    "- `hidden_dim`: the size of the LSTM's hidden and cell states.\n",
    "- `n_layers`: the number of stacked LSTM layers used in the decoder.\n",
    "- `dropout`: the dropout probability applied to the embedded input vectors.\n",
    "\n",
    "Internally, the decoder uses `nn.Embedding`, `nn.LSTM`, and `nn.Linear` layers to implement the core logic. The dropout mechanism is realized with `nn.Dropout`. These components are standard in PyTorch and can be configured flexibly for different architectures. The output of the decoder at each step is a prediction over the vocabulary space and a pair of updated states that are used in the subsequent step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, embedding_dim, hidden_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(output_dim, embedding_dim)\n",
    "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=dropout)\n",
    "        self.fc_out = nn.Linear(hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input, hidden, cell):\n",
    "        input = input.unsqueeze(0)\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
    "        prediction = self.fc_out(output.squeeze(0))\n",
    "        return prediction, hidden, cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "08b91566-7a53-4b4b-98a8-aba5a2f7f9f0"
   },
   "source": [
    "## Seq2Seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qt6okDz9avXI"
   },
   "source": [
    "## Sequence-to-Sequence (Seq2Seq) Model\n",
    "\n",
    "The `Seq2Seq` class represents the full sequence-to-sequence architecture that combines the encoder and decoder into a single end-to-end model. It is implemented by subclassing `torch.nn.Module`, and manages the data flow between the encoder and decoder during training or inference.\n",
    "\n",
    "Upon initialization, the model checks that the encoder and decoder are structurally compatible — specifically, that they have equal hidden dimensionality and the same number of recurrent layers. This is essential for correctly passing the encoder's final hidden and cell states as the initial states for the decoder.\n",
    "\n",
    "During the forward pass, the model receives two inputs:\n",
    "- `src`: a tensor containing the tokenized source sentence (Xhosa/Zulu), shaped as \\([ \\text{src\\_len}, \\text{batch\\_size} ]\\)\n",
    "- `trg`: a tensor containing the ground-truth target sentence (English), shaped as \\([ \\text{trg\\_len}, \\text{batch\\_size} ]\\)\n",
    "\n",
    "The model first encodes the `src` sequence using the encoder. The resulting hidden and cell states are used to initialize the decoder. The decoder is then run iteratively over the length of the target sequence, starting with the `<sos>` token as the first input.\n",
    "\n",
    "At each time step \\( t \\), the decoder receives three inputs:\n",
    "1. The input token for the current step\n",
    "2. The hidden state from the previous step\n",
    "3. The cell state from the previous step\n",
    "\n",
    "The decoder produces a prediction over the target vocabulary. This prediction is stored in a tensor `outputs` which accumulates the results for all time steps.\n",
    "\n",
    "To improve learning, the model uses *teacher forcing*, a technique where the actual target token is used as the next input instead of the model's own prediction. This is controlled by the `teacher_forcing_ratio`, a probability value that determines how often ground-truth tokens are used during training. If teacher forcing is disabled at a given step, the model uses its predicted token (`top1`) as the next input.\n",
    "\n",
    "This architecture enables the model to learn mappings from input to output sequences in a supervised manner, effectively modeling complex dependencies across languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "        assert encoder.hidden_dim == decoder.hidden_dim, \\\n",
    "            \"Hidden dimensions of encoder and decoder must be equal!\"\n",
    "        assert encoder.n_layers == decoder.n_layers, \\\n",
    "            \"Encoder and decoder must have equal number of layers!\"\n",
    "\n",
    "    def forward(self, src, trg, teacher_forcing_ratio):\n",
    "        batch_size = trg.shape[1]\n",
    "        trg_length = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "\n",
    "        outputs = torch.zeros(trg_length, batch_size, trg_vocab_size).to(self.device)\n",
    "\n",
    "        hidden, cell = self.encoder(src)\n",
    "        input = trg[0, :]\n",
    "\n",
    "        for t in range(1, trg_length):\n",
    "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
    "            outputs[t] = output\n",
    "\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            top1 = output.argmax(1)\n",
    "            input = trg[t] if teacher_force else top1\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "63aekFltBNpI"
   },
   "source": [
    "## Model Instantiation\n",
    "\n",
    "The model was instantiated with carefully selected hyperparameters that balance representational capacity and computational feasibility. The source and target vocabulary sizes were derived from the previously constructed vocabularies: `input_dim` corresponds to the size of the Xhosa/Zulu vocabulary, and `output_dim` to that of the English vocabulary.\n",
    "\n",
    "Both the encoder and decoder used an embedding dimensionality of 256, which is a common setting that captures semantic relationships without incurring excessive computational cost. The hidden state dimensionality was set to 512, allowing the LSTM units to store richer sequential information. Two recurrent layers (`n_layers = 2`) were used in both the encoder and decoder to enable hierarchical feature extraction, which has been shown to improve generalization in deep sequence modeling tasks (Sutskever et al., 2014).\n",
    "\n",
    "Dropout regularization was applied independently in the encoder and decoder, each with a dropout probability of 0.5. This helps reduce overfitting by randomly deactivating neurons during training (Srivastava et al., 2014).\n",
    "\n",
    "The encoder and decoder were instantiated separately and then passed into the `Seq2Seq` wrapper class. The complete model was moved to GPU (if available) using PyTorch’s `torch.device()` interface. This setup allows for efficient tensor operations and training acceleration on CUDA-compatible hardware.\n",
    "\n",
    "This configuration serves as the baseline LSTM model. All subsequent training and evaluation are performed using this architecture unless otherwise specified.\n",
    "\n",
    "**References:**  \n",
    "Sutskever, I., Vinyals, O., & Le, Q. V. (2014). Sequence to sequence learning with neural networks. *Advances in Neural Information Processing Systems*, 27.  \n",
    "Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., & Salakhutdinov, R. (2014). Dropout: A simple way to prevent neural networks from overfitting. *Journal of Machine Learning Research*, 15(1), 1929–1958.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = len(zu_vocab)\n",
    "output_dim = len(en_vocab)\n",
    "encoder_embedding_dim = 256\n",
    "decoder_embedding_dim = 256\n",
    "hidden_dim = 512\n",
    "n_layers = 2\n",
    "encoder_dropout = 0.5\n",
    "decoder_dropout = 0.5\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "encoder = Encoder(\n",
    "    input_dim,\n",
    "    encoder_embedding_dim,\n",
    "    hidden_dim,\n",
    "    n_layers,\n",
    "    encoder_dropout,\n",
    ")\n",
    "\n",
    "decoder = Decoder(\n",
    "    output_dim,\n",
    "    decoder_embedding_dim,\n",
    "    hidden_dim,\n",
    "    n_layers,\n",
    "    decoder_dropout,\n",
    ")\n",
    "\n",
    "model = Seq2Seq(encoder, decoder, device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
    "\n",
    "\n",
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "print(f\"The model has {count_parameters(model):,} trainable parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "77VOn7Vic7xI"
   },
   "source": [
    "## Optimization and Loss Function\n",
    "\n",
    "The model is trained using the Adam optimizer, a widely used adaptive learning rate algorithm that combines the benefits of RMSProp and momentum (Kingma and Ba, 2015). Adam is particularly effective for training deep neural networks on noisy or sparse gradients, as is often the case in natural language processing.\n",
    "\n",
    "For the loss function, we employ the categorical cross-entropy loss (`nn.CrossEntropyLoss`) to measure the difference between the predicted and actual target tokens at each time step. This loss function is suitable for multi-class classification tasks where each output corresponds to a probability distribution over a vocabulary. Importantly, we configure the loss function to ignore the `<pad>` token index using the `ignore_index` parameter. This ensures that padded positions in the target sequences do not contribute to the loss during training, thereby preventing the model from learning to predict padding tokens and focusing instead on meaningful content.\n",
    "\n",
    "Together, the optimizer and loss function form the foundation of the training loop, enabling the model to update its parameters via backpropagation.\n",
    "\n",
    "**Reference:**  \n",
    "Kingma, D. P., & Ba, J. (2015). Adam: A method for stochastic optimization. *International Conference on Learning Representations (ICLR)*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "# loss function\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a29UzKgAd0AM"
   },
   "source": [
    "## Training Procedure\n",
    "\n",
    "The model was trained using a mini-batch gradient descent loop implemented in the `train_fn` function. This function executes a forward and backward pass through the model for each batch in the training set and updates the model’s parameters using the Adam optimizer.\n",
    "\n",
    "At each iteration, the input (`src`) and target (`trg`) sequences are retrieved from the batch and moved to the appropriate device (GPU or CPU). The model generates predicted output sequences using the current parameters and the specified `teacher_forcing_ratio`, which determines the probability of using ground-truth tokens as input at each decoding step. The loss is computed using the categorical cross-entropy criterion, excluding padded tokens via the `ignore_index` setting.\n",
    "\n",
    "To avoid exploding gradients — a common issue in training recurrent neural networks — we apply gradient clipping using PyTorch’s `clip_grad_norm_` function. This ensures that the norm of the gradients stays within a specified threshold (`clip`). After computing gradients via backpropagation, the optimizer updates the model parameters.\n",
    "\n",
    "The function returns the average loss over all batches in the epoch, providing a quantitative measure of model performance during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(model, data_loader, optimizer, criterion, clip, teacher_forcing_ratio, device):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for batch in data_loader:\n",
    "        src = batch[\"zu_ids\"].to(device)\n",
    "        trg = batch[\"en_ids\"].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, trg, teacher_forcing_ratio)\n",
    "\n",
    "        output_dim = output.shape[-1]\n",
    "        output = output[1:].view(-1, output_dim)\n",
    "        trg = trg[1:].view(-1)\n",
    "\n",
    "        loss = criterion(output, trg)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(data_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "txA5zs5ueX-U"
   },
   "source": [
    "## Evaluation Procedure\n",
    "\n",
    "Model evaluation is performed using a function (`evaluate_fn`) that closely mirrors the training loop, but with key modifications to ensure that the model is assessed under inference-like conditions. Specifically, the model is switched to evaluation mode using `model.eval()`, which disables dropout layers and other training-specific behaviors. Additionally, all operations are wrapped in a `torch.no_grad()` context to prevent gradient computations, reducing memory usage and improving computational efficiency.\n",
    "\n",
    "During evaluation, the decoder receives no ground-truth tokens; instead, it generates each token based solely on its previous predictions. This is achieved by setting the `teacher_forcing_ratio` to zero. The model outputs are reshaped to match the dimensions expected by the cross-entropy loss function, which computes the discrepancy between the predicted and actual target sequences. The loss is accumulated over all batches and averaged to yield a single scalar metric that reflects the model’s performance on the validation or test set.\n",
    "\n",
    "This evaluation strategy provides a reliable estimate of the model’s generalization ability without influencing the model’s parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_fn(model, data_loader, criterion, device):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            src = batch[\"zu_ids\"].to(device)\n",
    "            trg = batch[\"en_ids\"].to(device)\n",
    "\n",
    "            output = model(src, trg, 0)  # no teacher forcing\n",
    "            output_dim = output.shape[-1]\n",
    "            output = output[1:].view(-1, output_dim)\n",
    "            trg = trg[1:].view(-1)\n",
    "\n",
    "            loss = criterion(output, trg)\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(data_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jGBgCnKOfzjw"
   },
   "source": [
    "## Epoch-Level Training Loop and Checkpointing\n",
    "\n",
    "Model training is conducted over multiple epochs, with each epoch consisting of a complete pass through the training dataset. The number of epochs (`n_epochs`) was set to 10, and a gradient clipping threshold (`clip = 1.0`) was used to constrain the norm of gradients during backpropagation, reducing the risk of unstable updates.\n",
    "\n",
    "During each epoch, the model undergoes training using the `train_fn` function, followed by evaluation on the validation set via `evaluate_fn`. The average losses from both procedures are recorded. To assess model confidence and track convergence more effectively, perplexity scores were computed by exponentiating the average loss values:\n",
    "\n",
    "$$\\text{Perplexity} = \\exp(\\text{Loss})$$\n",
    "\n",
    "Perplexity provides an interpretable metric for language modeling, where lower values indicate better predictive performance.\n",
    "\n",
    "A model checkpointing mechanism was incorporated to save the best-performing model based on validation loss. Specifically, the model's parameters are saved to disk (`tut1-model.pt`) only if the current epoch achieves a lower validation loss than any previous epoch. This ensures that the saved model generalizes well to unseen data.\n",
    "\n",
    "All training and evaluation losses, as well as their corresponding perplexities, are stored in lists for post-training visualization. This allows for later plotting of learning curves, which is useful for diagnosing underfitting, overfitting, or training instability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 10\n",
    "clip = 1.0\n",
    "teacher_forcing_ratio = 0.5\n",
    "\n",
    "best_valid_loss = float(\"inf\")\n",
    "\n",
    "# log variables for graph visualizations\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "train_ppls = []\n",
    "valid_ppls = []\n",
    "\n",
    "for epoch in tqdm.tqdm(range(n_epochs)):\n",
    "    train_loss = train_fn(\n",
    "        model,\n",
    "        train_data_loader,\n",
    "        optimizer,\n",
    "        criterion,\n",
    "        clip,\n",
    "        teacher_forcing_ratio,\n",
    "        device,\n",
    "    )\n",
    "    valid_loss = evaluate_fn(\n",
    "        model,\n",
    "        valid_data_loader,\n",
    "        criterion,\n",
    "        device,\n",
    "    )\n",
    "    # save the model only if validation loss improves\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), \"tut1-model.pt\")\n",
    "\n",
    "    # perplexity calculation\n",
    "    train_ppl = np.exp(train_loss)\n",
    "    valid_ppl = np.exp(valid_loss)\n",
    "\n",
    "    # log and track\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    train_ppls.append(train_ppl)\n",
    "    valid_ppls.append(valid_ppl)\n",
    "\n",
    "    print(f\"\\tTrain Loss: {train_loss:7.3f} | Train PPL: {train_ppl:7.3f}\")\n",
    "    print(f\"\\tValid Loss: {valid_loss:7.3f} | Valid PPL: {valid_ppl:7.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iZXym9S1GIia"
   },
   "source": [
    "## Graphs and Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = range(1, n_epochs + 1)\n",
    "\n",
    "# plot Loss\n",
    "plt.figure()\n",
    "plt.plot(epochs, train_losses, label='Train Loss')\n",
    "plt.plot(epochs, valid_losses, label='Valid Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('[LSTM] Loss Curve')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# plot Perplexity\n",
    "plt.figure()\n",
    "plt.plot(epochs, train_ppls, label='Train PPL')\n",
    "plt.plot(epochs, valid_ppls, label='Valid PPL')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Perplexity')\n",
    "plt.title('[LSTM] Perplexity Curve')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zPjmQOShCdr4"
   },
   "source": [
    "## RNN Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"tut1-model.pt\"))\n",
    "\n",
    "test_loss = evaluate_fn(model, test_data_loader, criterion, device)\n",
    "\n",
    "print(f\"| Test Loss: {test_loss:.3f} | Test PPL: {np.exp(test_loss):7.3f} |\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EdjTcrVniM9D"
   },
   "source": [
    "## Inference: Translating Sentences\n",
    "\n",
    "To evaluate the model qualitatively, we define a function `translate_sentence()` that takes a raw Xhosa or Zulu input sentence and returns a translated English sentence using greedy decoding.\n",
    "\n",
    "The function performs the following steps:\n",
    "\n",
    "1. **Tokenization and Preprocessing**  \n",
    "   The input sentence is tokenized using the SpaCy tokenizer (same as used during training). Tokens are lowercased (if enabled), and wrapped with start (`<sos>`) and end (`<eos>`) tokens. These tokens are then converted to integer indices via the source vocabulary and converted into a PyTorch tensor.\n",
    "\n",
    "2. **Encoding**  \n",
    "   The token indices are passed into the encoder to obtain the initial hidden and cell states that summarize the source sentence.\n",
    "\n",
    "3. **Greedy Decoding**  \n",
    "   The decoder is initialized with the `<sos>` token and generates one token at a time. At each time step, the token with the highest probability is selected using `argmax`, and the decoding process continues until the `<eos>` token is predicted or a maximum output length is reached.\n",
    "\n",
    "4. **Output Postprocessing**  \n",
    "   The output token indices are mapped back into readable English words using the target vocabulary.\n",
    "\n",
    "The decoding process is performed without teacher forcing, thus simulating a real inference scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentence(\n",
    "    sentence,\n",
    "    model,\n",
    "    en_nlp,\n",
    "    zu_nlp,\n",
    "    en_vocab,\n",
    "    zu_vocab,\n",
    "    lower,\n",
    "    sos_token,\n",
    "    eos_token,\n",
    "    device,\n",
    "    max_output_length=25,\n",
    "):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        if isinstance(sentence, str):\n",
    "            tokens = [token.text for token in zu_nlp.tokenizer(sentence)]\n",
    "        else:\n",
    "            tokens = [token for token in sentence]\n",
    "        if lower:\n",
    "            tokens = [token.lower() for token in tokens]\n",
    "        tokens = [sos_token] + tokens + [eos_token]\n",
    "        ids = zu_vocab.lookup_indices(tokens)\n",
    "        tensor = torch.LongTensor(ids).unsqueeze(-1).to(device)\n",
    "        hidden, cell = model.encoder(tensor)\n",
    "        inputs = en_vocab.lookup_indices([sos_token])\n",
    "        for _ in range(max_output_length):\n",
    "            inputs_tensor = torch.LongTensor([inputs[-1]]).to(device)\n",
    "            output, hidden, cell = model.decoder(inputs_tensor, hidden, cell)\n",
    "            predicted_token = output.argmax(-1).item()\n",
    "            inputs.append(predicted_token)\n",
    "            if predicted_token == en_vocab[eos_token]:\n",
    "                break\n",
    "        tokens = en_vocab.lookup_tokens(inputs)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_data[0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = test_data[0][\"xh\"]\n",
    "expected_translation = test_data[0][\"en\"]\n",
    "\n",
    "sentence, expected_translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translations = [\n",
    "    translate_sentence(\n",
    "        example[\"xh\"],\n",
    "        model,\n",
    "        en_nlp,\n",
    "        zu_nlp,\n",
    "        en_vocab,\n",
    "        zu_vocab,\n",
    "        lower,\n",
    "        sos_token,\n",
    "        eos_token,\n",
    "        device,\n",
    "    )\n",
    "    for example in tqdm.tqdm(test_data)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu = evaluate.load(\"bleu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format both the model predictions and ground-truth references appropriately\n",
    "predictions = [\" \".join(translation[1:-1]) for translation in translations]\n",
    "\n",
    "references = [[example[\"en\"]] for example in test_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions[0], references[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer Wrapper Function\n",
    "def get_tokenizer_fn(nlp, lower):\n",
    "    def tokenizer_fn(s):\n",
    "        tokens = [token.text for token in nlp.tokenizer(s)]\n",
    "        if lower:\n",
    "            tokens = [token.lower() for token in tokens]\n",
    "        return tokens\n",
    "\n",
    "    return tokenizer_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_fn = get_tokenizer_fn(en_nlp, lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_fn(predictions[0]), tokenizer_fn(references[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = bleu.compute(\n",
    "    predictions=predictions, references=references, tokenizer=tokenizer_fn\n",
    ")\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "80B6pBm5uo-L"
   },
   "source": [
    "# Attention Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QdWHHlc8khCw"
   },
   "source": [
    "## Attention Mechanism\n",
    "\n",
    "To enhance the decoder's ability to focus on relevant parts of the source sequence during translation, we incorporate an attention mechanism. This module computes a weighted sum over the encoder's outputs at each decoding step, allowing the model to dynamically attend to different parts of the input.\n",
    "\n",
    "The attention module follows a formulation similar to the additive attention mechanism, as described in Bahdanau et al. (2015). It takes the current decoder hidden state and the full sequence of encoder outputs to compute attention scores, which are then normalized via a softmax layer.\n",
    "\n",
    "### Key Components\n",
    "\n",
    "- `self.attn`: A linear layer that combines the decoder's hidden state and the encoder output at each time step.\n",
    "- `self.v`: A learnable parameter vector used to convert attention energies into scalar scores via a dot product.\n",
    "\n",
    "### Forward Pass Description\n",
    "\n",
    "1. The decoder's last-layer hidden state (`hidden`) is repeated across the source length dimension to align with `encoder_outputs`.\n",
    "2. Both are concatenated and passed through a non-linear activation (tanh) and a linear projection.\n",
    "3. The attention energies are then computed as a dot product with the learnable parameter `v`.\n",
    "4. A softmax is applied to obtain attention weights over the source tokens.\n",
    "\n",
    "This results in a context vector that can be used by the decoder to selectively incorporate information from the source sequence, enabling more accurate translations, particularly for longer or syntactically complex sentences.\n",
    "\n",
    "This design is consistent with the alignment-based methods described by Jurafsky and Martin (2021) and widely used in modern sequence-to-sequence architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.attn = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        self.v = nn.Parameter(torch.rand(hidden_dim))\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        if hidden.dim() == 3:\n",
    "            hidden = hidden[-1]  # [batch_size, hidden_dim]\n",
    "\n",
    "        src_len = encoder_outputs.shape[0]  # [src_len, batch_size, hidden_dim]\n",
    "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)  # [batch_size, src_len, hidden_dim]\n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)  # [batch_size, src_len, hidden_dim]\n",
    "\n",
    "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))  # [batch_size, src_len, hidden_dim]\n",
    "        energy = energy.permute(0, 2, 1)  # [batch_size, hidden_dim, src_len]\n",
    "\n",
    "        v = self.v.repeat(encoder_outputs.size(0), 1).unsqueeze(1)  # [batch_size, 1, hidden_dim]\n",
    "        attention = torch.bmm(v, energy).squeeze(1)  # [batch_size, src_len]\n",
    "        return torch.softmax(attention, dim=1)  # [batch_size, src_len]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HXE8kvS2kydR"
   },
   "source": [
    "## Attention-Based Decoder\n",
    "\n",
    "To complement the encoder with a context-aware decoding process, we implement an attention-based decoder (`AttnDecoder`). This module extends the baseline LSTM decoder by integrating a dynamic context vector derived from the attention mechanism.\n",
    "\n",
    "### Architectural Overview\n",
    "\n",
    "- **Embedding Layer**: Converts target token indices into dense vector representations.\n",
    "- **Attention Layer**: Computes attention weights between the decoder's current hidden state and all encoder outputs. This allows the decoder to \"attend\" to different parts of the source sequence dynamically at each time step.\n",
    "- **Context Vector (Weighted Sum)**: A weighted sum of encoder outputs is calculated based on attention scores, producing a context vector.\n",
    "- **LSTM Input**: The embedded input token is concatenated with the context vector and passed into the LSTM.\n",
    "- **Linear Output Projection**: The output of the LSTM is combined with the context vector and passed through a linear layer to generate token scores over the target vocabulary.\n",
    "\n",
    "### Forward Pass Semantics\n",
    "\n",
    "1. The decoder receives the previous token’s index, current hidden and cell states, and the full sequence of encoder outputs.\n",
    "2. The attention scores are computed by comparing the current decoder state with each encoder output.\n",
    "3. These scores are used to compute a weighted context vector via batch matrix multiplication.\n",
    "4. The LSTM is fed the concatenation of the current embedded token and the context vector.\n",
    "5. The decoder then predicts the next token using a linear transformation of the concatenated LSTM output and context vector.\n",
    "\n",
    "This structure allows the decoder to align with the most relevant source tokens at each generation step, improving translation fidelity — especially in long or syntactically complex sequences.\n",
    "\n",
    "This approach reflects the attention-enhanced sequence models described in Jurafsky and Martin (2021), and builds on the additive attention method from Bahdanau et al. (2015).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnDecoder(nn.Module):\n",
    "    def __init__(self, output_dim, embedding_dim, hidden_dim, n_layers, dropout, attention):\n",
    "        super().__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.attention = attention\n",
    "        self.embedding = nn.Embedding(output_dim, embedding_dim)\n",
    "        self.rnn = nn.LSTM(embedding_dim + hidden_dim, hidden_dim, n_layers, dropout=dropout)\n",
    "        self.fc_out = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input, hidden, cell, encoder_outputs):\n",
    "        input = input.unsqueeze(0)\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        a = self.attention(hidden[-1], encoder_outputs)\n",
    "        a = a.unsqueeze(1)\n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "        weighted = torch.bmm(a, encoder_outputs)\n",
    "        weighted = weighted.permute(1, 0, 2)\n",
    "        rnn_input = torch.cat((embedded, weighted), dim=2)\n",
    "        output, (hidden, cell) = self.rnn(rnn_input, (hidden, cell))\n",
    "        output = output.squeeze(0)\n",
    "        weighted = weighted.squeeze(0)\n",
    "        prediction = self.fc_out(torch.cat((output, weighted), dim=1))\n",
    "        return prediction, hidden, cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gu3eccMOk-o9"
   },
   "source": [
    "## Attention-Based Sequence-to-Sequence Model\n",
    "\n",
    "The `AttnSeq2Seq` class integrates an attention-equipped decoder into the traditional sequence-to-sequence architecture. This allows the decoder to dynamically reference specific encoder outputs during generation, instead of relying solely on the final hidden state.\n",
    "\n",
    "### Model Structure\n",
    "\n",
    "- **Encoder**: Processes the input sequence and outputs both the final hidden states and the full sequence of intermediate outputs, which are needed for attention.\n",
    "- **Attention Decoder**: At each time step, the decoder uses the current hidden state to compute attention weights over the encoder outputs. This enables context-aware decoding.\n",
    "- **Device Configuration**: The model is configured to run on either CPU or GPU, depending on availability.\n",
    "\n",
    "### Forward Pass Workflow\n",
    "\n",
    "1. **Initialization**: A tensor `outputs` is initialized to store token-level predictions at each time step.\n",
    "2. **Encoding**: The input sequence is passed through the encoder to obtain hidden and cell states. The full sequence of encoder outputs is also retained to support attention.\n",
    "3. **Decoding**:\n",
    "   - The decoder begins with the `<sos>` token as input.\n",
    "   - For each time step, the decoder uses the current input token, the hidden and cell states, and the encoder outputs to generate:\n",
    "     - The next output prediction (token scores).\n",
    "     - Updated hidden and cell states.\n",
    "   - Teacher forcing is optionally applied: with probability `teacher_forcing_ratio`, the true token is used as the next input; otherwise, the model's own prediction is used.\n",
    "\n",
    "### Purpose and Benefit\n",
    "\n",
    "This architecture allows the decoder to make informed decisions by attending to different parts of the source sentence. It is especially beneficial in handling long-range dependencies, which standard encoder-decoder models struggle with. This setup closely mirrors the architecture proposed by Bahdanau et al. (2015) and is grounded in the broader framework of attention-based neural machine translation described in Jurafsky and Martin (2021)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnSeq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "        batch_size = trg.shape[1]\n",
    "        trg_len = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
    "        hidden, cell = self.encoder(src)\n",
    "        encoder_outputs, _ = self.encoder.rnn(self.encoder.embedding(src))\n",
    "        input = trg[0, :]\n",
    "        for t in range(1, trg_len):\n",
    "            output, hidden, cell = self.decoder(input, hidden, cell, encoder_outputs)\n",
    "            outputs[t] = output\n",
    "            top1 = output.argmax(1)\n",
    "            input = trg[t] if random.random() < teacher_forcing_ratio else top1\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "97PeZVmIlfkS"
   },
   "source": [
    "## Training the Attention-Based Model\n",
    "\n",
    "To evaluate the benefits of attention, we train a second sequence-to-sequence model that incorporates an attention mechanism within the decoder.\n",
    "\n",
    "### Model Initialization\n",
    "\n",
    "We instantiate the following components:\n",
    "\n",
    "- `Attention`: The attention module is initialized using the `hidden_dim` from the LSTM layers.\n",
    "- `AttnDecoder`: The decoder is extended with an attention mechanism that receives both the current decoder state and the encoder outputs at each time step.\n",
    "- `AttnSeq2Seq`: The encoder is combined with the attention-equipped decoder into a unified model.\n",
    "\n",
    "Weight initialization is applied using a custom `init_weights` function to ensure consistent parameter scaling before training. The loss function and optimizer remain unchanged from the non-attention model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention = Attention(hidden_dim)\n",
    "decoder_attn = AttnDecoder(\n",
    "    output_dim, decoder_embedding_dim, hidden_dim, n_layers, decoder_dropout, attention\n",
    ")\n",
    "model_attn = AttnSeq2Seq(encoder, decoder_attn, device).to(device)\n",
    "model_attn.apply(init_weights)\n",
    "optimizer_attn = optim.Adam(model_attn.parameters())\n",
    "criterion_attn = nn.CrossEntropyLoss(ignore_index=pad_index)\n",
    "\n",
    "train_losses_attn = []\n",
    "valid_losses_attn = []\n",
    "train_ppls_attn = []\n",
    "valid_ppls_attn = []\n",
    "\n",
    "best_valid_loss_attn = float(\"inf\")\n",
    "\n",
    "for epoch in tqdm.tqdm(range(n_epochs)):\n",
    "    train_loss = train_fn(model_attn, train_data_loader, optimizer_attn, criterion_attn, clip, teacher_forcing_ratio, device)\n",
    "    valid_loss = evaluate_fn(model_attn, valid_data_loader, criterion_attn, device)\n",
    "\n",
    "    if valid_loss < best_valid_loss_attn:\n",
    "        best_valid_loss_attn = valid_loss\n",
    "        torch.save(model_attn.state_dict(), \"attn-model.pt\")\n",
    "\n",
    "    train_ppl = np.exp(train_loss)\n",
    "    valid_ppl = np.exp(valid_loss)\n",
    "\n",
    "    train_losses_attn.append(train_loss)\n",
    "    valid_losses_attn.append(valid_loss)\n",
    "    train_ppls_attn.append(train_ppl)\n",
    "    valid_ppls_attn.append(valid_ppl)\n",
    "\n",
    "    print(f\"[Attn] Train Loss: {train_loss:.3f} | Train PPL: {train_ppl:.3f}\")\n",
    "    print(f\"[Attn] Valid Loss: {valid_loss:.3f} | Valid PPL: {valid_ppl:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u0OapnccwKBn"
   },
   "source": [
    "## Graphs and Visualizations\n",
    "To assess the training dynamics of the attention-enhanced model, we plot both the loss and perplexity curves over the full training period. These visualizations provide insights into model convergence, generalization, and potential overfitting or underfitting.\n",
    "\n",
    "### Loss Curve\n",
    "\n",
    "The loss values from both the training and validation sets are plotted across all epochs. A smooth, decreasing validation loss generally indicates successful learning, while a divergence between training and validation loss may suggest overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = range(1, n_epochs + 1)\n",
    "\n",
    "# plot Loss\n",
    "plt.figure()\n",
    "plt.plot(epochs, train_losses_attn, label='Train Loss')\n",
    "plt.plot(epochs, valid_losses_attn, label='Valid Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('[ATTN] Loss Curve')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# plot Perplexity\n",
    "plt.figure()\n",
    "plt.plot(epochs, train_ppls_attn, label='Train PPL')\n",
    "plt.plot(epochs, valid_ppls_attn, label='Valid PPL')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Perplexity')\n",
    "plt.title('[ATTN] Perplexity Curve')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xdw4Q9xhUxOh"
   },
   "source": [
    "## BLEU Evaluation with Attention Model\n",
    "\n",
    "To quantitatively assess translation performance, we apply BLEU (Bilingual Evaluation Understudy) scoring to the outputs of the attention-based model. BLEU is a widely used metric for evaluating the quality of machine-translated text by comparing n-gram overlap between predicted translations and human references.\n",
    "\n",
    "### Inference Process\n",
    "\n",
    "We define a translation function that takes an Xhosa/Zulu sentence and returns an English prediction:\n",
    "\n",
    "- The input sentence is tokenized and converted to indices using the source vocabulary.\n",
    "- The encoder processes the input to generate context-rich hidden states.\n",
    "- The decoder generates the output sequence one token at a time, using the attention mechanism to focus on relevant source positions.\n",
    "\n",
    "Predictions are collected from the model, and the true English references are retrieved from the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best attention model weights\n",
    "model_attn.load_state_dict(torch.load(\"attn-model.pt\"))\n",
    "model_attn.eval()\n",
    "\n",
    "# Translate using the trained model\n",
    "def translate_with_attention(example):\n",
    "    sentence = example[\"xh\"]\n",
    "    tokens = [token.text for token in zu_nlp.tokenizer(sentence)]\n",
    "    if lower:\n",
    "        tokens = [t.lower() for t in tokens]\n",
    "    tokens = [sos_token] + tokens + [eos_token]\n",
    "    ids = zu_vocab.lookup_indices(tokens)\n",
    "    src_tensor = torch.LongTensor(ids).unsqueeze(-1).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        embedded = model_attn.encoder.embedding(src_tensor)\n",
    "        encoder_outputs, (hidden, cell) = model_attn.encoder.rnn(embedded)\n",
    "        input_token = torch.tensor([en_vocab[sos_token]]).to(device)\n",
    "\n",
    "        predicted = []\n",
    "        for _ in range(50):  # max output length\n",
    "            output, hidden, cell = model_attn.decoder(\n",
    "                input_token,\n",
    "                hidden,\n",
    "                cell,\n",
    "                encoder_outputs\n",
    "            )\n",
    "\n",
    "            pred_token = output.argmax(-1).item()\n",
    "            if pred_token == en_vocab[eos_token]:\n",
    "                break\n",
    "            predicted.append(pred_token)\n",
    "            input_token = torch.tensor([pred_token]).to(device)\n",
    "\n",
    "    pred_tokens = en_vocab.lookup_tokens(predicted)\n",
    "    return \" \".join(pred_tokens)\n",
    "\n",
    "\n",
    "# Collect predictions and references\n",
    "attn_predictions = [translate_with_attention(ex) for ex in test_data]\n",
    "attn_references = [[ex[\"en\"]] for ex in test_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_predictions[0], attn_references[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute BLEU\n",
    "bleu_result_attn = bleu.compute(predictions=attn_predictions, references=attn_references, tokenizer=tokenizer_fn)\n",
    "\n",
    "bleu_result_attn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YWHHx3eOFSTZ"
   },
   "source": [
    "# Pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model and tokenizer\n",
    "m2m_model_name = \"facebook/m2m100_418M\"\n",
    "m2m_tokenizer = M2M100Tokenizer.from_pretrained(m2m_model_name)\n",
    "m2m_model = M2M100ForConditionalGeneration.from_pretrained(m2m_model_name).to(device)\n",
    "\n",
    "# Set source language\n",
    "m2m_tokenizer.src_lang = \"xh\"  # or \"zu\" for Zulu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translation function\n",
    "def translate_with_m2m100(sentence):\n",
    "    # Tokenize the input sentence\n",
    "    encoded = m2m_tokenizer(sentence, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # Generate translation\n",
    "    generated_tokens = m2m_model.generate(\n",
    "        **encoded,\n",
    "        forced_bos_token_id=m2m_tokenizer.get_lang_id(\"en\"),  # Force target lang to English\n",
    "        max_length=50\n",
    "    )\n",
    "\n",
    "    # Decode the output\n",
    "    translation = m2m_tokenizer.decode(generated_tokens[0], skip_special_tokens=True)\n",
    "    return translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m2m_predictions = [\n",
    "    translate_with_m2m100(example[\"xh\"]) for example in test_data\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 100\n",
    "\n",
    "print(f\"Attn:\\n{attn_predictions[index]}\\nm2m:\\n{m2m_predictions[index]}\\n\")\n",
    "print(f\"reference:\\n{attn_references[index]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu_result_m2m = bleu.compute(predictions=m2m_predictions, references=attn_references)\n",
    "bleu_result_m2m"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
